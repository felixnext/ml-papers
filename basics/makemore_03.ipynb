{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Makemore - Manual Backprop\n",
    "\n",
    "This will take the existing network, but build it into a low level class where we run every step manually (like in `micrograd` notebooks).\n",
    "\n",
    "We will also use the set of debug functions created previously to debug and check the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/research/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchviz import make_dot\n",
    "from typing import List, Callable, Dict, Any, Union, Optional, Tuple\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HParams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = 5\n",
    "emb_size = 64\n",
    "hidden_layer = 512"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Loading & Prep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of names:  32033\n",
      "Train: 25626\n",
      "Dev: 3203\n",
      "Eval: 3204\n",
      "Vocab Size: 27\n",
      "torch.Size([182513, 5])\n",
      "torch.Size([22720, 5])\n",
      "torch.Size([22913, 5])\n"
     ]
    }
   ],
   "source": [
    "# load the name data\n",
    "with open(\"data/names.txt\") as f:\n",
    "    names = f.read().splitlines()\n",
    "\n",
    "# print stats\n",
    "print(\"Number of names: \", len(names))\n",
    "\n",
    "# split names into bigrams\n",
    "bigrams = {}\n",
    "for name in names:\n",
    "    ls = [\"<T>\"] + list(name.lower()) + [\"<T>\"]\n",
    "    tpl = list(zip(ls, ls[1:]))\n",
    "    for bigram in tpl:\n",
    "        bigrams[bigram] = bigrams.get(bigram, 0) + 1\n",
    "items = sorted(list(set([b for a, b in bigrams.keys()])))\n",
    "pos_map = {v: k for k, v in enumerate(items)}\n",
    "num_items = len(items)\n",
    "t_bigrams = torch.zeros((num_items, num_items))\n",
    "\n",
    "\n",
    "# split names into 3 datasets (based on percentages)\n",
    "train_perc, dev_perc, eval_perc = 0.8, 0.1, 0.1\n",
    "\n",
    "# shuffle and split names\n",
    "random.shuffle(names)\n",
    "train_names = names[: int(len(names) * train_perc)]\n",
    "dev_names = names[\n",
    "    int(len(names) * train_perc) : int(len(names) * (train_perc + dev_perc))\n",
    "]\n",
    "eval_names = names[int(len(names) * (train_perc + dev_perc)) :]\n",
    "\n",
    "# print stats\n",
    "print(f\"Train: {len(train_names)}\")\n",
    "print(f\"Dev: {len(dev_names)}\")\n",
    "print(f\"Eval: {len(eval_names)}\")\n",
    "\n",
    "\n",
    "def gen_dataset(items: List[str], encode: Callable[[str], List[int]], heads: int):\n",
    "    train = []\n",
    "    label = []\n",
    "    for name in items:\n",
    "        ls = [0] * heads + encode(name) + [0]\n",
    "        tpl = list(zip(ls, *[ls[i + 1 :] for i in range(heads)]))\n",
    "        for bigram in tpl:\n",
    "            train.append(bigram[:-1])\n",
    "            label.append(bigram[-1])\n",
    "\n",
    "    # convert to tensors and expand as one-hots\n",
    "    train = torch.tensor(train)\n",
    "    label = F.one_hot(torch.tensor(label), num_items)\n",
    "    return train, label\n",
    "\n",
    "\n",
    "def char_encoding(name: str) -> List[int]:\n",
    "    return [pos_map[i] for i in list(name.lower())]\n",
    "\n",
    "\n",
    "print(f\"Vocab Size: {num_items}\")\n",
    "\n",
    "num_mlp_items = len(pos_map)\n",
    "\n",
    "train_X, train_y = gen_dataset(train_names, char_encoding, heads=heads)\n",
    "dev_X, dev_y = gen_dataset(dev_names, char_encoding, heads=heads)\n",
    "eval_X, eval_y = gen_dataset(eval_names, char_encoding, heads=heads)\n",
    "print(train_X.shape)\n",
    "print(dev_X.shape)\n",
    "print(eval_X.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Debug Helpers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots the given model\n",
    "def plot_mlp(model, X=train_X):\n",
    "    return make_dot(model.forward(X), params=dict(model.named_parameters()))\n",
    "\n",
    "\n",
    "def compute_nll(model, X, y):\n",
    "    # compute the loss\n",
    "    probs = model.predict_proba(X)\n",
    "    lhood = (probs * y).sum(dim=1)\n",
    "    # take mean to make usre large data is still handlable\n",
    "    return -torch.log(lhood).mean()\n",
    "\n",
    "\n",
    "# function to compute cross entropy loss\n",
    "ce_loss = lambda p, y: F.cross_entropy(p, torch.argmax(y, dim=1))\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    # sum number of elements in all model parameters that take a gradient\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def grid_space_lr(exp_start, exp_end, num) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    # NOTE: this can be used by single training steps (random data) then check when the loss explodes\n",
    "    # generate a grid of learning rates\n",
    "    space = torch.linspace(exp_start, exp_end, num)\n",
    "    return 10**space, space\n",
    "\n",
    "\n",
    "def grid_search_lr(\n",
    "    model,\n",
    "    exp_start: int,\n",
    "    exp_end: int,\n",
    "    steps: int,\n",
    "    train_X: torch.Tensor,\n",
    "    train_y: torch.Tensor,\n",
    "    loss_fct: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
    "    grad_compute: Callable[[torch.Tensor], None],\n",
    "    batch_size: int = 1000,\n",
    "):\n",
    "    # generate grid space\n",
    "    lrs, lri = grid_space_lr(exp_start, exp_end, steps)\n",
    "\n",
    "    # train model for each learning rate\n",
    "    losses = []\n",
    "    for i in range(len(lrs)):\n",
    "        # sample from trainX and trainY\n",
    "        idx = torch.randint(0, train_X.shape[0], (batch_size,))\n",
    "        batch_X = train_X[idx]\n",
    "        batch_y = train_y[idx]\n",
    "\n",
    "        # compute loss\n",
    "        probs = model(batch_X)[0]\n",
    "        loss = loss_fct(probs, batch_y)\n",
    "        grad_compute(loss)\n",
    "\n",
    "        # optimize\n",
    "        for p in model.parameters():\n",
    "            p.data -= lrs[i] * p.grad.data\n",
    "            p.grad.data.zero_()\n",
    "\n",
    "        # compute loss\n",
    "        losses.append(loss.sum().item())\n",
    "\n",
    "    # plot data\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(lri, losses)\n",
    "    ax.set_xlabel(\"Learning Rate Exp\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"Loss vs Learning Rate Exp\")\n",
    "    plt.show()\n",
    "\n",
    "    return losses, lri\n",
    "\n",
    "def clean_stats():\n",
    "    return {\n",
    "        \"batch_loss\": [],\n",
    "        \"train_loss\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"epoch_steps\": [],\n",
    "    }\n",
    "\n",
    "def plot_stats(stats: dict):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(\n",
    "        np.arange(len(stats[\"batch_loss\"])), stats[\"batch_loss\"], label=\"Batch Loss\"\n",
    "    )\n",
    "    ax.plot(stats[\"epoch_steps\"], stats[\"train_loss\"], label=\"Train Loss\")\n",
    "    ax.plot(stats[\"epoch_steps\"], stats[\"test_loss\"], label=\"Test Loss\")\n",
    "    ax.set_xlabel(\"Batch Steps\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "# init the weights and verify how\n",
    "def init_weights(model: nn.Module, init_func):\n",
    "    for p in model.parameters():\n",
    "        init_func(p)\n",
    "\n",
    "def uniform_init(p: nn.Parameter):\n",
    "    torch.nn.init.uniform_(p.data, -1, 1)\n",
    "\n",
    "def fixed_init(p: nn.Parameter):\n",
    "    p.data.fill_(0.01)\n",
    "\n",
    "def he_init(p: nn.Parameter):\n",
    "    # NOTE: to compute this dynamically we would need have access to the lower layers\n",
    "    if p.dim() > 1:\n",
    "        torch.nn.init.kaiming_uniform_(p.data, a=0, mode=\"fan_in\", nonlinearity=\"tanh\")\n",
    "    else:\n",
    "        torch.nn.init.uniform_(p.data, -1, 1)\n",
    "\n",
    "def weight_stats(model: nn.Module):\n",
    "    for p in model.parameters():\n",
    "        print(f\"Weight Mean: {p.data.mean()} - Weight Std: {p.data.std()}\")\n",
    "\n",
    "def visualize_activation(model: nn.Module, train_X: torch.Tensor, bins: int=100):\n",
    "    # perform a forward pass\n",
    "    probs = model.forward(train_X)\n",
    "    # model out should be list of tuples\n",
    "    layer_data = model.out[-2][1]\n",
    "\n",
    "    # compute the results\n",
    "    act_list = layer_data.view(-1).detach().numpy()\n",
    "    plt.hist(act_list, bins=bins)\n",
    "    plt.show()\n",
    "\n",
    "    # also do advanced visualization\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(layer_data.abs() > 0.99, cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "def visualize_forward(model, X, y, loss_fct, grad_compute, ud):\n",
    "    logits = model.forward(X)\n",
    "    data = dict(model.out)\n",
    "\n",
    "    # create two stacked plots\n",
    "    fig = plt.subplots(figsize=(20, 20))\n",
    "    ax = plt.subplot(4, 1, 1)\n",
    "\n",
    "    # plot the logits\n",
    "    ax.set_title(\"Activations\")\n",
    "    for name in data:\n",
    "        tdata = torch.histogram(data[name], density=True)\n",
    "        ax.plot(tdata[1][:-1].detach(), tdata[0].detach(), label=name)\n",
    "        data[name].retain_grad()\n",
    "    # set limits on x axis\n",
    "    ax.set_xbound(-2, 2)\n",
    "    ax.legend()\n",
    "\n",
    "    # go back\n",
    "    loss = loss_fct(logits, y)\n",
    "    grad_compute(loss)\n",
    "\n",
    "    # get gradient data\n",
    "    ax = plt.subplot(4, 1, 2)\n",
    "    ax.set_title(\"Data Gradients\")\n",
    "    for name in data:\n",
    "        tdata = torch.histogram(data[name].grad, density=True)\n",
    "        ax.plot(tdata[1][:-1].detach(), tdata[0].detach(), label=name)\n",
    "    ax.set_xbound(-.01, .01)\n",
    "    ax.legend()\n",
    "\n",
    "    # iterate model parameters\n",
    "    ax = plt.subplot(4, 1, 3)\n",
    "    ax.set_title(\"Model Gradients\")\n",
    "    for p in model.parameters():\n",
    "        t = p.grad\n",
    "        if t is None or t.ndim < 2 or t.shape[0] == 1:\n",
    "            continue\n",
    "        print(f\"{t.shape} grad to data {t.std() / p.std()}\")\n",
    "        tdata = torch.histogram(t, density=True)\n",
    "        ax.plot(tdata[1][:-1].detach(), tdata[0].detach(), label=f\"{tuple(t.shape)}\")\n",
    "    ax.set_xbound(-.01, .01)\n",
    "    ax.legend()\n",
    "\n",
    "    # iterate model parameters\n",
    "    ax = plt.subplot(4, 1, 4)\n",
    "    ax.set_title(\"LR dep Weight Updates\")\n",
    "    for i, p in enumerate(model.parameters()):\n",
    "        if p.ndim == 2:\n",
    "            ax.plot([model.ud[j][i] for j in range(len(model.ud))], label=f\"param {i}\")\n",
    "    ax.plot([0, len(model.ud)], [-3, -3], 'k', label='baseline')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-create our basic model\n",
    "\n",
    "We want to build our model from scratch by reimplementing all the different layers and operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for @: 'Tensor' and 'Linear'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[199], line 282\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39m# create model\u001b[39;00m\n\u001b[1;32m    281\u001b[0m model \u001b[39m=\u001b[39m MLPModel(num_items, heads, emb_size, hidden_layer, bn_momentum\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n\u001b[0;32m--> 282\u001b[0m logs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(train_X[:\u001b[39m10\u001b[39;49m])\n\u001b[1;32m    283\u001b[0m \u001b[39mprint\u001b[39m(logs\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    284\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel Parameters: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39msum\u001b[39m([p\u001b[39m.\u001b[39mnumel()\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mp\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39mmodel\u001b[39m.\u001b[39mparameters()])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[199], line 227\u001b[0m, in \u001b[0;36mMLPModel.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout\u001b[39m.\u001b[39mappend((\u001b[39m\"\u001b[39m\u001b[39membs\u001b[39m\u001b[39m\"\u001b[39m, embs))\n\u001b[1;32m    226\u001b[0m \u001b[39m# compute hidden layer and apply batchnorm\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m hidden \u001b[39m=\u001b[39m embs \u001b[39m@\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_hidden1\n\u001b[1;32m    228\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout\u001b[39m.\u001b[39mappend((\u001b[39m\"\u001b[39m\u001b[39mhidden1_mul\u001b[39m\u001b[39m\"\u001b[39m, hidden))\n\u001b[1;32m    229\u001b[0m bn_mean \u001b[39m=\u001b[39m hidden\u001b[39m.\u001b[39mmean(\u001b[39m0\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bnrmean1\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for @: 'Tensor' and 'Linear'"
     ]
    }
   ],
   "source": [
    "class Module():\n",
    "    def __init__(self):\n",
    "        self.out = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        # applies a forward step, computing the output of the layer\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, grad):\n",
    "        # applies a backward step, computing gradients based on gradient of previous layers\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def learn(self, lr):\n",
    "        # applies a gradient step to the internal gradients and zeros them again\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._in = None\n",
    "        self._fw = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.out = []\n",
    "        self._in = x\n",
    "        self._fw = torch.tanh(x)\n",
    "        self.out.append((\"tanh\", x))\n",
    "        return self._fw\n",
    "\n",
    "    def backward(self, grad):\n",
    "        fw_grad = grad * (1 - torch.tanh(self._in) ** 2)\n",
    "        return fw_grad\n",
    "    \n",
    "    def learn(self, lr):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_size: int, out_size: int, bias: bool=True, scale: float = 1.0, bias_scale: float = 1.0, g: torch.Generator = None):\n",
    "        super().__init__()\n",
    "        self._in = None\n",
    "        self._mul_fw = None\n",
    "        self._bias = bias\n",
    "        self._w = torch.randn((in_size, out_size), generator=g) * scale\n",
    "        if bias:\n",
    "            self._bias_fw = None\n",
    "            self._b = torch.randn((out_size,), generator=g) * bias_scale\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.out = []\n",
    "        self._in = x\n",
    "        # execute mat_mul operation\n",
    "        self._mul_fw = x @ self._w\n",
    "        out = self._mul_fw\n",
    "        if self._bias:\n",
    "            self._bias_fw = out + self._b\n",
    "            out = self._bias_fw\n",
    "        self.out.append((\"linear\", out))\n",
    "        return out\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        # compute gradient of bias first (as it modulates remainder)\n",
    "        if self._bias:\n",
    "            # note: input data here is the output of self._mat_mul - since this is just addition, gradient gets distributed equally\n",
    "            self._b.grad = grad.sum(dim=0)\n",
    "        # overall function here is Lin(X) = X @ W\n",
    "        # compute gradient of the weights (dLin / dW) (this then modulates the input by the gradient)\n",
    "        self._w.grad = self._in.t() @ grad\n",
    "        # compute gradient of the input (dLin / dX)\n",
    "        fw_grad = grad @ self._w.t()\n",
    "        return fw_grad\n",
    "    \n",
    "    def learn(self, lr):\n",
    "        # apply gradient step\n",
    "        self._w -= lr * self._w.grad\n",
    "        self._w.grad = None\n",
    "\n",
    "        # check for bias\n",
    "        if self._bias:\n",
    "            self._b -= lr * self._b.grad\n",
    "            self._b.grad = None\n",
    "    \n",
    "class CrossEntropy(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._fw = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.out = []\n",
    "        self.out.append((\"ce_in\", x))\n",
    "        self._in = x\n",
    "        # NOTE: at its core cross-entropy compares two distributions (in this case the one-hot encoded label and the softmax output)\n",
    "        # softmax is just a normalization function, so we can compute the cross-entropy by comparing the two distributions\n",
    "        # formula for softmax: softmax(x) = exp(x) / sum(exp(x))\n",
    "        self._xe_fw = torch.exp(x)\n",
    "        self._xsum_fw = torch.sum(self._xe_fw, dim=1, keepdim=True)\n",
    "        self._sm_fw = self._xe_fw * (self._xsum_fw**-1)\n",
    "        self._y_fw = y\n",
    "        # now compute the cross-entropy\n",
    "        self._loss_fw = -torch.sum(self._y_fw * torch.log(self._sm_fw), dim=1)\n",
    "        # take the mean, as we want to boil down to single loss\n",
    "        #self._fw = torch.sum(self._loss_fw * (self._loss_fw.numel()**-1))\n",
    "        self._fw = torch.mean(self._loss_fw)\n",
    "        # self._fw = torch.nn.functional.cross_entropy(x, y)\n",
    "        self.out.append((\"ce_out\", x))\n",
    "        return self._fw\n",
    "\n",
    "    def backward(self, grad):\n",
    "        # compute gradient of the mean (and expand to number of batches) [] to [n]\n",
    "        grad = grad / self._loss_fw.numel()\n",
    "        # NOTE: sum distributes the gradient, through the number of classes [n] to [n, c]\n",
    "        # both can be done through broadcasting from point loss\n",
    "        sm_grad = self._sm_fw\n",
    "        sm_grad[self._y_fw == 1] -= 1\n",
    "\n",
    "        # grad = grad * (self._y_fw / self._sm_fw)\n",
    "        # compute gradient through the softmax layer\n",
    "        #sm_grad = ((self._xe_fw * (self._xsum_fw - self._xe_fw)) / (self._xsum_fw**2.0))\n",
    "        return grad * sm_grad\n",
    "    \n",
    "    def learn(self, lr):\n",
    "        pass\n",
    "\n",
    "class BatchNorm(Module):\n",
    "    def __init__(self, in_size: int, momentum: float=0.9, eps: float=1e-5):\n",
    "        super().__init__()\n",
    "        self._in = None\n",
    "        self._fw = None\n",
    "        self._eps = eps\n",
    "        self._mom = momentum\n",
    "        self._mean = torch.zeros((1, in_size,))\n",
    "        self._var = torch.ones((1, in_size,))\n",
    "        self._gamma = torch.ones((1, in_size,))\n",
    "        self._beta = torch.zeros((1, in_size,))\n",
    "        self._n = 0\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, train: bool=True):\n",
    "        self.out = []\n",
    "        self._in = x\n",
    "        # compute mean and variance\n",
    "        mean = torch.mean(x, dim=0, keepdim=True) if train else self._mean\n",
    "        var = torch.var(x, dim=0, keepdim=True, unbiased=False) if train else self._var\n",
    "        # update running mean and variance\n",
    "        if train:\n",
    "            self._mean = self._mom * self._mean + (1 - self._mom) * torch.mean(x, dim=1, keepdim=True)\n",
    "            self._var = self._mom * self._var + (1 - self._mom) * torch.var(x, dim=1, keepdim=True)\n",
    "        # apply batch norm\n",
    "        self._sqrt_fw = torch.sqrt(var + self._eps)\n",
    "        self._mean_fw = mean\n",
    "        self._std_fw = (x - mean)\n",
    "        self._fw = ((self._gamma * self._std_fw) / self._sqrt_fw) + self._beta\n",
    "        self.out.append((\"bn\", self._fw))\n",
    "        return self._fw\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        # apply gradient directly to the beta\n",
    "        self._beta.grad = grad.sum(dim=0, keepdim=True)\n",
    "\n",
    "        # compute gradient of the gamma\n",
    "        self._gamma.grad = (grad * (self._std_fw / self._sqrt_fw)).sum(dim=0, keepdim=True)\n",
    "\n",
    "        # FIXME: this is incorrect (does not contain all branches of data)\n",
    "        # compute gradient of the input\n",
    "        #fw_grad = ((self._gamma - (self._gamma/self._mean_fw)) * (self._std_fw) - (self._gamma * self._std_fw) * (2/self._in.numel()) * self._std_fw * (1/(2*self._sqrt_fw))) / (self._sqrt_fw**2)\n",
    "        N = self._in.numel()\n",
    "        fw_grad = self._gamma * (1/self._sqrt_fw - 1/N + (2*self._std_fw**2)/N)\n",
    "        return fw_grad * grad\n",
    "    \n",
    "    def learn(self, lr):\n",
    "        # apply gradient step\n",
    "        self._gamma -= lr * self._gamma.grad\n",
    "        self._gamma.grad = None\n",
    "        self._beta -= lr * self._beta.grad\n",
    "        self._beta.grad = None\n",
    "\n",
    "class MLPModel(Module):\n",
    "    def __init__(self, num_items: int, heads: int, emb_size: int, hidden_size: int, bn_momentum: float=0.9, seed: int=42):\n",
    "        # generate to make predictable\n",
    "        g = torch.Generator()\n",
    "        if seed is not None:\n",
    "            g = g.manual_seed(seed)\n",
    "\n",
    "        # embedding weights\n",
    "        self._embs = torch.randn((num_items, emb_size), generator=g)\n",
    "        # layer 1 weights\n",
    "        self._hidden1 = Linear(emb_size * heads, hidden_size, scale=(5/3)/((emb_size * heads)**0.5), g=g) # he init\n",
    "        self._bn1 = BatchNorm(hidden_size, momentum=bn_momentum)\n",
    "        self._hidden2 = Linear(hidden_size, hidden_size, scale=(5/3)/((hidden_size)**0.5), g=g) # he init\n",
    "        self._bn2 = BatchNorm(hidden_size, momentum=bn_momentum)\n",
    "        self._predict = Linear(hidden_size, num_items, bias=True, scale=0.1, bias_scale=0.1, g=g)\n",
    "\n",
    "        # previous data\n",
    "        # self._hidden1 = torch.randn((emb_size * heads, hidden_size), generator=g) * (5/3)/((emb_size * heads)**0.5) # he init\n",
    "        # self._bngains1 = torch.ones((1, hidden_size))\n",
    "        # self._bnbias1 = torch.zeros((1, hidden_size))\n",
    "        # self._bnrmean1 = torch.zeros(hidden_size)\n",
    "        # self._bnrvar1 = torch.ones(hidden_size)\n",
    "        # # layer 2 weights\n",
    "        # self._hidden2 = torch.randn((hidden_size, hidden_size), generator=g) * (5/3)/((hidden_size)**0.5) # he init\n",
    "        # self._bngains2 = torch.ones((1, hidden_size))\n",
    "        # self._bnbias2 = torch.zeros((1, hidden_size))\n",
    "        # self._bnrmean2 = torch.zeros(hidden_size)\n",
    "        # self._bnrvar2 = torch.ones(hidden_size)\n",
    "        # # layer 3 weights\n",
    "        # self._predict = torch.randn((hidden_size, num_items), generator=g) * 0.1\n",
    "        # self._predict_bias = torch.randn(num_items, generator=g) * 0.1\n",
    "\n",
    "        # create some helper vars\n",
    "        self.out = []\n",
    "        # self.bn_momentum = bn_momentum\n",
    "        self._train = True\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.forward(X)\n",
    "    \n",
    "    @property\n",
    "    def train(self):\n",
    "        return self._train\n",
    "\n",
    "    @train.setter\n",
    "    def train(self, value):\n",
    "        self._train = value\n",
    "    \n",
    "    def forward(self, X: torch.Tensor):\n",
    "        # check if X needs to be expanded\n",
    "        X = X.unsqueeze(0) if X.ndim == 1 else X\n",
    "        self.out = [(\"X\", X)]\n",
    "\n",
    "        # retrieve and combine embeddings\n",
    "        embs = self._embs[X]\n",
    "        embs = embs.view(embs.shape[0], -1)\n",
    "        self.out.append((\"embs\", embs))\n",
    "\n",
    "        # compute hidden layer and apply batchnorm\n",
    "        hidden = embs @ self._hidden1\n",
    "        self.out.append((\"hidden1_mul\", hidden))\n",
    "        bn_mean = hidden.mean(0, keepdim=True) if self._train else self._bnrmean1\n",
    "        bn_var = hidden.var(0, keepdim=True) if self._train else self._bnrvar1\n",
    "        hidden = self._bngains1 * ((hidden - bn_mean) / torch.sqrt(bn_var + 1e-5)) + self._bnbias1\n",
    "        self.out.append((\"hidden1\", hidden))\n",
    "        if self._train:\n",
    "            self._bnrmean1 = self.bn_momentum * bn_mean + (1 - self.bn_momentum) * self._bnrmean1\n",
    "            self._bnrvar1 = self.bn_momentum * bn_var + (1 - self.bn_momentum) * self._bnrvar1\n",
    "        hidden = torch.tanh(hidden)\n",
    "        self.out.append((\"hidden1_act\", hidden))\n",
    "        \n",
    "        # compute hidden layer and apply batchnorm\n",
    "        hidden = hidden @ self._hidden2\n",
    "        self.out.append((\"hidden2_mul\", hidden))\n",
    "        bn_mean = hidden.mean(0, keepdim=True) if self._train else self._bnrmean2\n",
    "        bn_var = hidden.var(0, keepdim=True) if self._train else self._bnrvar2\n",
    "        hidden = self._bngains2 * ((hidden - bn_mean) / torch.sqrt(bn_var + 1e-5)) + self._bnbias2\n",
    "        self.out.append((\"hidden2\", hidden))\n",
    "        if self._train:\n",
    "            self._bnrmean2 = self.bn_momentum * bn_mean + (1 - self.bn_momentum) * self._bnrmean2\n",
    "            self._bnrvar2 = self.bn_momentum * bn_var + (1 - self.bn_momentum) * self._bnrvar2\n",
    "        hidden = torch.tanh(hidden)\n",
    "        self.out.append((\"hidden2_act\", hidden))\n",
    "\n",
    "        # compute logits\n",
    "        logits = hidden @ self._predict + self._predict_bias\n",
    "        self.out.append((\"logits\", logits))\n",
    "        return logits\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        # compute the gradients for each part of the network and return the input gradient\n",
    "        # NOTE: in general we do not really care about that input gradient\n",
    "        pass\n",
    "\n",
    "    def predict(self, X: torch.Tensor):\n",
    "        probs = self.predict_proba(X)\n",
    "        return torch.argmax(probs, dim=1)\n",
    "    \n",
    "    def predict_proba(self, X: torch.Tensor):\n",
    "        logits = self.forward(X)\n",
    "        return F.softmax(logits, dim=1)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self._embs, self._hidden1, self._bngains1, self._bnbias1, self._hidden2, self._bngains2, self._bnbias2, self._predict]\n",
    "    \n",
    "    def named_parameters(self):\n",
    "        return dict(embs=self._embs, hidden1=self._hidden1, bngains1=self._bngains1, bnbias1=self._bnbias1, hidden2=self._hidden2, bngains2=self._bngains2, bnbias2=self._bnbias2, predict=self._predict)\n",
    "    \n",
    "    def reset_grads(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = None\n",
    "\n",
    "# create model\n",
    "model = MLPModel(num_items, heads, emb_size, hidden_layer, bn_momentum=0.1)\n",
    "logs = model.forward(train_X[:10])\n",
    "print(logs.shape)\n",
    "print(f\"Model Parameters: {sum([p.numel() for p in model.parameters()])}\")\n",
    "\n",
    "# plot activations\n",
    "visualize_activation(model, train_X[:100])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to build an optimize and a backward function.\n",
    "\n",
    "Keep in mind that chain rule for a chain of functions $f(x) = f_2(f_1(x))$ is:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial f_2} \\frac{\\partial f_2}{\\partial f_1} \\frac{\\partial f_1}{\\partial x}$$\n",
    "\n",
    "A simpler case of $L(x) = y(x)$ is:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial x}$$\n",
    "\n",
    "Where $\\frac{\\partial L}{\\partial x}$ is the gradient of the loss with respect to $x$, i.e. how much does x influence the value of the loss (since we want to minimize loss we want to move into the negative direction here). For the starting point $\\frac{\\partial L}{\\partial y}$ has a value of 1, since we want to start with the gradient of the loss with respect to the output of the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(pred: torch.Tensor, y: torch.Tensor, loss_fct: Callable):\n",
    "    # compute the loss\n",
    "    loss: torch.Tensor = loss_fct(pred, y)\n",
    "\n",
    "    # start with the gradients of the loss, Function is L(x, y) = f_1(f_net(x), y)\n",
    "    # as chain rule the derivative of L/dx = dL/df_1 * df_1/df_net * df_net/dx\n",
    "    # we have the dL/df_1 = 1, df_1/df_net is the derivative of the loss function\n",
    "    loss.grad = torch.ones_like(loss)\n",
    "    pred.grad = loss.grad * grad()\n",
    "    \n",
    "    # compute the gradients manually\n",
    "    # chain rule: dL/dW = dL/dY * dY/dW\n",
    "\n",
    "def optimize(model: MLPModel, stats) -> List:\n",
    "    # TODO: integrate stats and build the ud list\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "Some additonal gradient computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]], grad_fn=<AddBackward0>)\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]], grad_fn=<AddBackward0>)\n",
      "tensor([[30., 36., 42.],\n",
      "        [66., 81., 96.]], grad_fn=<MmBackward0>)\n",
      "tensor([[5., 5., 5.],\n",
      "        [7., 7., 7.],\n",
      "        [9., 9., 9.]])\n",
      "tensor([[5., 5., 5.],\n",
      "        [7., 7., 7.],\n",
      "        [9., 9., 9.]], grad_fn=<MmBackward0>)\n",
      "tensor([[ 6., 15., 24.],\n",
      "        [ 6., 15., 24.]])\n",
      "tensor([[ 6., 15., 24.],\n",
      "        [ 6., 15., 24.]], grad_fn=<MmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(6, requires_grad=True, dtype=torch.float32).view(2, 3) + 1\n",
    "X.retain_grad()\n",
    "W = torch.arange(9, requires_grad=True, dtype=torch.float32).view(3, 3) + 1\n",
    "W.retain_grad()\n",
    "\n",
    "print(X)\n",
    "print(W)\n",
    "\n",
    "O = X @ W\n",
    "print(O)\n",
    "O.backward(torch.ones_like(O))\n",
    "print(W.grad)\n",
    "print(X.t() @ torch.ones_like(O))\n",
    "print(X.grad)\n",
    "print(torch.ones_like(O) @ W.t())\n",
    "\n",
    "jjjO.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BatchNorm\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]], grad_fn=<AddBackward0>)\n",
      "> Manually\n",
      "tensor([[-1.0000, -1.0000, -1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000]], grad_fn=<AddBackward0>)\n",
      "tensor([[1.2500, 1.2500, 1.2500],\n",
      "        [1.2500, 1.2500, 1.2500]], grad_fn=<MulBackward0>)\n",
      "> Autograd\n",
      "tensor([[-1.0000, -1.0000, -1.0000],\n",
      "        [ 1.0000,  1.0000,  1.0000]], grad_fn=<NativeBatchNormBackward0>)\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Test Entropy\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]], grad_fn=<AddBackward0>)\n",
      "> Manually\n",
      "tensor(1.9076, grad_fn=<MeanBackward0>)\n",
      "tensor([[-0.4550,  0.1224,  0.3326],\n",
      "        [ 0.0450, -0.3776,  0.3326]], grad_fn=<MulBackward0>)\n",
      "> Autograd\n",
      "tensor(1.9076, grad_fn=<MeanBackward0>)\n",
      "tensor([[-0.4550,  0.1224,  0.3326],\n",
      "        [ 0.0450, -0.3776,  0.3326]])\n"
     ]
    }
   ],
   "source": [
    "def compare(mod: Callable[[int], Module], fct: Callable[[torch.Tensor], torch.Tensor]):\n",
    "    th = mod(3)\n",
    "    print(f\"Test {th.__class__.__name__}\")\n",
    "    # Test the layers\n",
    "    X = torch.arange(6, requires_grad=True, dtype=torch.float32).view(2, 3) + 1\n",
    "    X.retain_grad()\n",
    "    print(X)\n",
    "    y = th.forward(X)\n",
    "    print(\"> Manually\")\n",
    "    print(y)\n",
    "    grad = th.backward(torch.ones_like(y))\n",
    "    print(grad)\n",
    "\n",
    "    # compute with autograd\n",
    "    print(\"> Autograd\")\n",
    "    X = torch.arange(6, requires_grad=True, dtype=torch.float32).view(2, 3) + 1\n",
    "    X.retain_grad()\n",
    "    y = fct(X)\n",
    "    print(y)\n",
    "    y.backward(torch.ones_like(y))\n",
    "    print(X.grad)\n",
    "\n",
    "#compare(lambda s: Tanh(), lambda X: torch.tanh(X))\n",
    "g1 = torch.Generator()\n",
    "g1.manual_seed(42)\n",
    "g2 = torch.Generator()\n",
    "g2.manual_seed(42)\n",
    "#compare(lambda s: Linear(s, s, g=g1), lambda X: X @ torch.randn((3, 3), dtype=torch.float32, generator=g2) + 1)\n",
    "bn = nn.BatchNorm1d(3, momentum=0.1)\n",
    "compare(lambda s: BatchNorm(s, momentum=0.9, eps=0.00001), lambda X: bn(X))\n",
    "\n",
    "# check cross entropy\n",
    "print(f\"Test Entropy\")\n",
    "# Test the layers\n",
    "X = torch.arange(6, requires_grad=True, dtype=torch.float32).view(2, 3) + 1\n",
    "Y = torch.tensor([0, 1], dtype=torch.long)\n",
    "Y = torch.nn.functional.one_hot(Y, num_classes=3).float()\n",
    "X.retain_grad()\n",
    "print(X)\n",
    "th = CrossEntropy()\n",
    "y = th.forward(X, Y)\n",
    "print(\"> Manually\")\n",
    "print(y)\n",
    "grad = th.backward(torch.ones_like(y))\n",
    "print(grad)\n",
    "\n",
    "# compute with autograd\n",
    "print(\"> Autograd\")\n",
    "X = torch.arange(6, requires_grad=True, dtype=torch.float32).view(2, 3) + 1\n",
    "Y = torch.tensor([0, 1], dtype=torch.long)\n",
    "Y = torch.nn.functional.one_hot(Y, num_classes=3).float()\n",
    "X.retain_grad()\n",
    "th = CrossEntropy()\n",
    "y = th.forward(X, Y)\n",
    "# y = F.cross_entropy(X, Y)\n",
    "print(y)\n",
    "y.backward(torch.ones_like(y))\n",
    "print(X.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7fe1a4422759eafe6868ccc54152cc839a79337f6f12136ad2d52cc24128f952"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
