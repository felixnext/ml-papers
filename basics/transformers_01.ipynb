{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model\n",
    "\n",
    "This tutorial will build a transformer model with a proper test-harness aroudn it.\n",
    "The model will be trained on top of tiny shakespear dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/research/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchviz import make_dot\n",
    "from typing import List, Callable, Dict, Any, Union, Optional, Tuple, Generator\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data**\n",
    "\n",
    "First step is to load the data and explore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Number of characters: 1115394\n",
      "Number of unique characters: 65\n",
      "Characters: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Res: [46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n"
     ]
    }
   ],
   "source": [
    "torch_device = torch.device(\"mps\" if torch.has_mps else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(f\"Using device: {torch_device}\")\n",
    "\n",
    "# read the tiny shakespear dataset\n",
    "with open('data/tiny_shakespeare.txt', 'r') as f:\n",
    "    lines = f.read()\n",
    "\n",
    "# read some stats\n",
    "print(f\"Number of characters: {len(lines)}\")\n",
    "chars = sorted(list(set(lines)))\n",
    "print(f\"Number of unique characters: {len(chars)}\")\n",
    "print(f\"Characters: {chars}\")\n",
    "\n",
    "# build a dict\n",
    "char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx2char = {i: ch for i, ch in enumerate(chars)}\n",
    "def encode_tensor(text: str) -> torch.Tensor:\n",
    "    return torch.tensor([char2idx[ch] for ch in text])\n",
    "\n",
    "def encode(text: str) -> List[int]:\n",
    "    return [char2idx[ch] for ch in text]\n",
    "\n",
    "def decode_tensor(encoded: torch.Tensor) -> str:\n",
    "    # assert dim\n",
    "    encoded = encoded.squeeze()\n",
    "    if encoded.dim() == 2:\n",
    "        encoded = encoded.argmax(dim=1)\n",
    "    if encoded.dim() > 1:\n",
    "        raise ValueError(f\"Expected 1D or 2D tensor, got {encoded.dim()}D tensor\")\n",
    "    return ''.join([idx2char[idx] for idx in encoded.tolist()])\n",
    "\n",
    "def decode(encoded: List[int]) -> str:\n",
    "    return ''.join([idx2char[idx] for idx in encoded])\n",
    "\n",
    "assert decode(encode(\"hello world\")) == \"hello world\"\n",
    "assert decode_tensor(encode_tensor(\"hello world\")) == \"hello world\"\n",
    "print(f\"Res: {encode('hello world')}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detour: Different Encodings\n",
    "\n",
    "Looking at a few different encoding libraries, in particular:\n",
    "\n",
    "* bpe - low level open source encoder\n",
    "* tokenizers - HuggingFaces implementation of tokenizers\n",
    "* sentencepiece - Google's implementation of tokenizers\n",
    "* tiktoken - OpenAIs production ready implementation of tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 80\n",
      "['__sow', 'he', 'l', 'lo', '__eow', 'world', '__sow', 'he', 'l', 'lo', '__eow']\n",
      "\n",
      "\n",
      "\n",
      "Vocabulary size: 100\n",
      "['h', 'e', 'll', 'o ', 'w', 'or', 'l', 'd']\n",
      "[50, 47, 88, 82, 65, 81, 54, 46]\n",
      "Result: [31373, 995] - type: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# do byte pair encoding\n",
    "from bpe import Encoder as BPEncoder\n",
    "\n",
    "bpe = BPEncoder(vocab_size=400)\n",
    "bpe.fit([lines])\n",
    "print(f\"Vocabulary size: {bpe.bpe_vocab_size}\")\n",
    "print(bpe.tokenize('hello world hello'))\n",
    "\n",
    "# try same with huggingface\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "tok = Tokenizer(BPE())\n",
    "trainer = BpeTrainer(vocab_size=100, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "tok.train([\"data/tiny_shakespeare.txt\"], trainer=trainer)\n",
    "print(f\"Vocabulary size: {tok.get_vocab_size()}\")\n",
    "print(tok.encode('hello world').tokens)\n",
    "print(tok.encode('hello world').ids)\n",
    "\n",
    "# use GPT2 tokenizer from tiktoken\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "# generates this automatically as a tensor\n",
    "res = enc.encode(\"hello world\")\n",
    "print(f\"Result: {res} - type: {type(res)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next building the dataset, by encoding everything into a tensor of integers and splitting into train test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 01003854\n",
      "Test data:  00111540\n"
     ]
    }
   ],
   "source": [
    "# encode the tensor\n",
    "data = encode_tensor(lines)\n",
    "\n",
    "# split the data\n",
    "split_idx = int(len(data) * 0.9)\n",
    "train_data = data[:split_idx]\n",
    "test_data = data[split_idx:]\n",
    "\n",
    "print(f\"Train data: {train_data.shape[0]:08}\")\n",
    "print(f\"Test data:  {test_data.shape[0]:08}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Batch ---\n",
      "X: torch.Size([4, 8])\n",
      "Y: torch.Size([4, 8])\n",
      "tensor([[43, 57, 58,  1, 16, 47, 39, 52],\n",
      "        [43, 11,  1, 50, 43, 58,  1, 51],\n",
      "        [46, 58,  8,  0,  0, 30, 27, 25],\n",
      "        [61, 53, 56, 51, 57,  8,  0, 35]])\n",
      "--- Walk ---\n",
      "tensor([43]) (torch.Size([1])) -> 57 (torch.Size([]))\n",
      "tensor([43, 57]) (torch.Size([2])) -> 58 (torch.Size([]))\n",
      "tensor([43, 57, 58]) (torch.Size([3])) -> 1 (torch.Size([]))\n",
      "tensor([43, 57, 58,  1]) (torch.Size([4])) -> 16 (torch.Size([]))\n",
      "tensor([43, 57, 58,  1, 16]) (torch.Size([5])) -> 47 (torch.Size([]))\n",
      "tensor([43, 57, 58,  1, 16, 47]) (torch.Size([6])) -> 39 (torch.Size([]))\n",
      "tensor([43, 57, 58,  1, 16, 47, 39]) (torch.Size([7])) -> 52 (torch.Size([]))\n",
      "tensor([43, 57, 58,  1, 16, 47, 39, 52]) (torch.Size([8])) -> 1 (torch.Size([]))\n",
      "tensor([43]) (torch.Size([1])) -> 11 (torch.Size([]))\n",
      "tensor([43, 11]) (torch.Size([2])) -> 1 (torch.Size([]))\n",
      "tensor([43, 11,  1]) (torch.Size([3])) -> 50 (torch.Size([]))\n",
      "tensor([43, 11,  1, 50]) (torch.Size([4])) -> 43 (torch.Size([]))\n",
      "tensor([43, 11,  1, 50, 43]) (torch.Size([5])) -> 58 (torch.Size([]))\n",
      "tensor([43, 11,  1, 50, 43, 58]) (torch.Size([6])) -> 1 (torch.Size([]))\n",
      "tensor([43, 11,  1, 50, 43, 58,  1]) (torch.Size([7])) -> 51 (torch.Size([]))\n",
      "tensor([43, 11,  1, 50, 43, 58,  1, 51]) (torch.Size([8])) -> 43 (torch.Size([]))\n",
      "tensor([46]) (torch.Size([1])) -> 58 (torch.Size([]))\n",
      "tensor([46, 58]) (torch.Size([2])) -> 8 (torch.Size([]))\n",
      "tensor([46, 58,  8]) (torch.Size([3])) -> 0 (torch.Size([]))\n",
      "tensor([46, 58,  8,  0]) (torch.Size([4])) -> 0 (torch.Size([]))\n",
      "tensor([46, 58,  8,  0,  0]) (torch.Size([5])) -> 30 (torch.Size([]))\n",
      "tensor([46, 58,  8,  0,  0, 30]) (torch.Size([6])) -> 27 (torch.Size([]))\n",
      "tensor([46, 58,  8,  0,  0, 30, 27]) (torch.Size([7])) -> 25 (torch.Size([]))\n",
      "tensor([46, 58,  8,  0,  0, 30, 27, 25]) (torch.Size([8])) -> 17 (torch.Size([]))\n",
      "tensor([61]) (torch.Size([1])) -> 53 (torch.Size([]))\n",
      "tensor([61, 53]) (torch.Size([2])) -> 56 (torch.Size([]))\n",
      "tensor([61, 53, 56]) (torch.Size([3])) -> 51 (torch.Size([]))\n",
      "tensor([61, 53, 56, 51]) (torch.Size([4])) -> 57 (torch.Size([]))\n",
      "tensor([61, 53, 56, 51, 57]) (torch.Size([5])) -> 8 (torch.Size([]))\n",
      "tensor([61, 53, 56, 51, 57,  8]) (torch.Size([6])) -> 0 (torch.Size([]))\n",
      "tensor([61, 53, 56, 51, 57,  8,  0]) (torch.Size([7])) -> 35 (torch.Size([]))\n",
      "tensor([61, 53, 56, 51, 57,  8,  0, 35]) (torch.Size([8])) -> 46 (torch.Size([]))\n",
      "--- Padded ---\n",
      "X: torch.Size([32, 8])\n",
      "Y: torch.Size([32])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0, 43],\n",
      "        [ 0,  0,  0,  0,  0,  0, 43, 57],\n",
      "        [ 0,  0,  0,  0,  0, 43, 57, 58],\n",
      "        [ 0,  0,  0,  0, 43, 57, 58,  1],\n",
      "        [ 0,  0,  0, 43, 57, 58,  1, 16],\n",
      "        [ 0,  0, 43, 57, 58,  1, 16, 47],\n",
      "        [ 0, 43, 57, 58,  1, 16, 47, 39],\n",
      "        [43, 57, 58,  1, 16, 47, 39, 52],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0, 43],\n",
      "        [ 0,  0,  0,  0,  0,  0, 43, 11],\n",
      "        [ 0,  0,  0,  0,  0, 43, 11,  1],\n",
      "        [ 0,  0,  0,  0, 43, 11,  1, 50],\n",
      "        [ 0,  0,  0, 43, 11,  1, 50, 43],\n",
      "        [ 0,  0, 43, 11,  1, 50, 43, 58],\n",
      "        [ 0, 43, 11,  1, 50, 43, 58,  1],\n",
      "        [43, 11,  1, 50, 43, 58,  1, 51],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0, 46],\n",
      "        [ 0,  0,  0,  0,  0,  0, 46, 58],\n",
      "        [ 0,  0,  0,  0,  0, 46, 58,  8],\n",
      "        [ 0,  0,  0,  0, 46, 58,  8,  0],\n",
      "        [ 0,  0,  0, 46, 58,  8,  0,  0],\n",
      "        [ 0,  0, 46, 58,  8,  0,  0, 30],\n",
      "        [ 0, 46, 58,  8,  0,  0, 30, 27],\n",
      "        [46, 58,  8,  0,  0, 30, 27, 25],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0, 61],\n",
      "        [ 0,  0,  0,  0,  0,  0, 61, 53],\n",
      "        [ 0,  0,  0,  0,  0, 61, 53, 56],\n",
      "        [ 0,  0,  0,  0, 61, 53, 56, 51],\n",
      "        [ 0,  0,  0, 61, 53, 56, 51, 57],\n",
      "        [ 0,  0, 61, 53, 56, 51, 57,  8],\n",
      "        [ 0, 61, 53, 56, 51, 57,  8,  0],\n",
      "        [61, 53, 56, 51, 57,  8,  0, 35]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# define the blocksize (length of context)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "# build a batch generator - this should sample random section from the text\n",
    "def random_batch(data: torch.Tensor, block_size: int, batch_size: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    '''Retrieves a random batch from the provided data'''\n",
    "    # check the input data\n",
    "    if data.dim() != 1:\n",
    "        raise ValueError(f\"Expected 1D tensor, got {data.dim()}D tensor\")\n",
    "\n",
    "    # create a random starting point\n",
    "    # note: we need to have one last char for output\n",
    "    # note: also need to have block_size difference\n",
    "    start_idx = torch.randint(0, len(data) - block_size - 1, (batch_size,))\n",
    "\n",
    "    # retrieve the content length (batchsi)\n",
    "    content = torch.stack([data[i:i + block_size + 1] for i in start_idx])\n",
    "    x = content[:, :-1]\n",
    "    y = content[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "def walk_batch(x: torch.Tensor, y: torch.Tensor) -> Generator[Tuple[torch.Tensor, torch.Tensor], None, None]:\n",
    "    '''Walk the batch by creating a list of x, y pairs'''\n",
    "    # perform a size check\n",
    "    if x.shape != y.shape:\n",
    "        raise ValueError(f\"Expected x and y to have same shape, got {x.shape} and {y.shape}\")\n",
    "    if x.dim() != 2:\n",
    "        raise ValueError(f\"Expected x and y to be 2D, got {x.dim()}D and {y.dim()}D\")\n",
    "    \n",
    "    # retrieve actual batch size and block size\n",
    "    batch_size, block_size = x.shape\n",
    "\n",
    "    # walk the batch\n",
    "    for b in range(batch_size):\n",
    "        for i in range(block_size):\n",
    "            bx = x[b, :i+1]\n",
    "            yield bx, y[b, i]\n",
    "\n",
    "def padded_walk_batch(x: torch.Tensor, y: torch.Tensor, pad: int = 0) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    '''Pads elements to max block_size and '''\n",
    "    # perform a size check\n",
    "    if x.shape != y.shape:\n",
    "        raise ValueError(f\"Expected x and y to have same shape, got {x.shape} and {y.shape}\")\n",
    "    if x.dim() != 2:\n",
    "        raise ValueError(f\"Expected x and y to be 2D, got {x.dim()}D and {y.dim()}D\")\n",
    "    \n",
    "    # retrieve actual batch size and block size\n",
    "    _, block_size = x.shape\n",
    "\n",
    "    _pad = lambda x: F.pad(x, (block_size - x.shape[-1], 0), mode=\"constant\", value=pad) \n",
    "    res = [(_pad(tx), ty) for tx, ty in walk_batch(x, y)]\n",
    "    return torch.stack([tpl[0] for tpl in res]).to(torch_device), torch.stack([tpl[1] for tpl in res]).to(torch_device)\n",
    "\n",
    "print(\"--- Batch ---\")\n",
    "x, y = random_batch(train_data, block_size, batch_size)\n",
    "print(f\"X: {x.shape}\")\n",
    "print(f\"Y: {y.shape}\")\n",
    "print(x)\n",
    "print(\"--- Walk ---\")\n",
    "gen = walk_batch(x, y)\n",
    "for wx, wy in gen:\n",
    "    print(f\"{wx} ({wx.shape}) -> {wy} ({wy.shape})\")\n",
    "print(\"--- Padded ---\")\n",
    "px, py = padded_walk_batch(x, y)\n",
    "print(f\"X: {px.shape}\")\n",
    "print(f\"Y: {py.shape}\")\n",
    "print(px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  6.,  8., 10.],\n",
       "        [12., 15., 18., 21.],\n",
       "        [24., 28., 32., 36.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance optimizations through tril\n",
    "tr = torch.tril(torch.ones((4, 4), dtype=torch.float32))\n",
    "print(tr)\n",
    "ar = torch.arange(16, dtype=torch.float32).view(4, 4)\n",
    "print(ar)\n",
    "tr @ ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n",
      "tensor([[-0.2356,  1.1298],\n",
      "        [-0.3788,  0.7653],\n",
      "        [ 0.3301, -1.7384],\n",
      "        [ 0.9123, -0.0854],\n",
      "        [-0.8835, -1.3622],\n",
      "        [ 0.7680, -0.1169],\n",
      "        [ 0.5364,  1.9739],\n",
      "        [-0.3348, -0.9146]])\n",
      "torch.Size([4, 8, 2])\n",
      "tensor([[-0.2356,  1.1298],\n",
      "        [-0.3072,  0.9476],\n",
      "        [-0.0948,  0.0522],\n",
      "        [ 0.1570,  0.0178],\n",
      "        [-0.0511, -0.2582],\n",
      "        [ 0.0854, -0.2346],\n",
      "        [ 0.1498,  0.0809],\n",
      "        [ 0.0892, -0.0436]])\n",
      "torch.Size([4, 8, 2])\n",
      "tensor([[-0.2356,  1.1298],\n",
      "        [-0.3072,  0.9476],\n",
      "        [-0.0948,  0.0522],\n",
      "        [ 0.1570,  0.0178],\n",
      "        [-0.0511, -0.2582],\n",
      "        [ 0.0854, -0.2346],\n",
      "        [ 0.1498,  0.0809],\n",
      "        [ 0.0892, -0.0436]])\n",
      "Data is same: True\n",
      "torch.Size([4, 8, 2])\n",
      "tensor([[-0.2356,  1.1298],\n",
      "        [-0.3072,  0.9476],\n",
      "        [-0.0948,  0.0522],\n",
      "        [ 0.1570,  0.0178],\n",
      "        [-0.0511, -0.2582],\n",
      "        [ 0.0854, -0.2346],\n",
      "        [ 0.1498,  0.0809],\n",
      "        [ 0.0892, -0.0436]])\n",
      "Data is same: True\n"
     ]
    }
   ],
   "source": [
    "# creating a tensor for self-attention of size (BATCH, TIME, CHANNELS)\n",
    "# Batch - number of data examples\n",
    "# Time - Context length we can look at\n",
    "# Channels - Number of features used\n",
    "sample = torch.randn(4, 8, 2)\n",
    "print(sample.shape)\n",
    "print(sample[0])\n",
    "\n",
    "# create an average up to the n-th time step (for a single example we could just use broadcasting on the batch dimension)\n",
    "B, T, C = sample.shape\n",
    "manual = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        # take the element from batch b and up to time t and mean it out\n",
    "        manual[b, t] = sample[b, :t+1].mean(dim=0)\n",
    "print(manual.shape)\n",
    "print(manual[0])\n",
    "\n",
    "# compute it but way more efficient\n",
    "tril = torch.tril(torch.ones((T, T), dtype=torch.float32))\n",
    "#print(tril)\n",
    "tril = tril / torch.arange(1, T+1, dtype=torch.float32).view(T, 1)\n",
    "#print(tril)\n",
    "auto1 = tril @ sample\n",
    "print(auto1.shape)\n",
    "print(auto1[0])\n",
    "\n",
    "print(f\"Data is same: {torch.allclose(manual, auto1)}\")\n",
    "\n",
    "# can do the same with softmax\n",
    "tril_mask = torch.tril(torch.ones((T, T), dtype=torch.float32))\n",
    "tril2 = torch.zeros((T, T), dtype=torch.float32)\n",
    "tril2 = tril2.masked_fill(tril_mask == 0, float('-inf'))\n",
    "tril2 = F.softmax(tril2, dim=-1)\n",
    "auto2 = tril2 @ sample\n",
    "print(auto2.shape)\n",
    "print(auto2[0])\n",
    "\n",
    "print(f\"Data is same: {torch.allclose(manual, auto2)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this will not give us a full training batch to work with\n",
    "\n",
    "**BiGram Language Model**\n",
    "\n",
    "This model has a content length of just one, so it can only predict the next char based on the previous one.\n",
    "This is done through an embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 1, 2, 3]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "([0] * block_size + [1, 2, 3])[-block_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.215925216674805\n",
      "Generated: iyYb!dN-a;aaagKl'qU-W,DHbvuoH;gQZrmh'vTHSPoZskBQ:?N$jQVIHwHCVXqSESEHUHRHSik.DK,jxwgbqPPob,wTeoIHanbR\n",
      "Params: 17729\n"
     ]
    }
   ],
   "source": [
    "class LModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def generate(self, max_len: int = 100) -> List[int]:\n",
    "        \"\"\"Generates a language output from the model\"\"\"\n",
    "        # set model to eval mode\n",
    "        self.eval()\n",
    "\n",
    "        # setup output\n",
    "        out = []\n",
    "        for i in range(max_len):\n",
    "            # setup tensor for generation\n",
    "            prev = torch.tensor((([0] * block_size) + out)[-block_size:], device=self.device, dtype=torch.long).view(1, -1)\n",
    "            # compute logits\n",
    "            logits = self.forward(prev)\n",
    "            # get the last logits and convert to prob\n",
    "            logits = logits[:, -1]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            sample = torch.multinomial(probs, 1)\n",
    "            # append to output\n",
    "            out.append(sample.detach().cpu().item())\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def generate_str(self, max_len: int = 100) -> str:\n",
    "        \"\"\"Generates a language output from the model\"\"\"\n",
    "        return \"\".join([idx2char[idx] for idx in self.generate(max_len=max_len)])\n",
    "\n",
    "class BigramLM(LModel):\n",
    "    def __init__(self, vocab_size, emb_size=128):\n",
    "        super().__init__(vocab_size)\n",
    "\n",
    "        # define the embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size) # (vocab_size, emb_size)\n",
    "        self.pos_embedding = nn.Embedding(block_size, emb_size) # (block_size, emb_size)\n",
    "        self.lm_head = nn.Linear(emb_size, vocab_size) # (emb_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # validate device\n",
    "        if self.device != x.device:\n",
    "            x = x.to(self.device)\n",
    "\n",
    "        # get ids\n",
    "        B, T = x.shape\n",
    "\n",
    "        # retrieve the embeddings\n",
    "        chars = self.embedding(x) # (B, block_size, emb_size)\n",
    "        pos = self.pos_embedding(torch.arange(T, device=x.device))  # (block_size, emb_size) - will be broadcases to (B, block_size, emb_size)\n",
    "        # add embeddings\n",
    "        embs = chars + pos # (B, block_size, emb_size)\n",
    "\n",
    "        # compute logits (push it to vocab_size)\n",
    "        # NOTE: we treat each B*block_size element as an independent example for which we predict the output\n",
    "        logits = self.lm_head(embs) # (B, block_size, vocab_size)\n",
    "        return logits\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        # validate device\n",
    "        if self.device != y.device:\n",
    "            y = y.to(self.device)\n",
    "\n",
    "        # compute the logits\n",
    "        logits = self.forward(x)  # (B, block_size, vocab_size)\n",
    "\n",
    "        # requires a reshape from (B, T, vocab_size) to (B*block_size, vocab_size)\n",
    "        logits = logits.view(-1, self.vocab_size)\n",
    "        target = y.reshape(-1) # (B*block_size)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = F.cross_entropy(logits, target) # (B*block_size, vocab_size), (B*block_size)\n",
    "        return loss\n",
    "\n",
    "# create model and test\n",
    "#dev = \"cpu\"\n",
    "dev = torch_device\n",
    "vocab_size = len(chars)\n",
    "model = BigramLM(vocab_size)\n",
    "model.to(dev)\n",
    "x, y = random_batch(train_data, block_size, batch_size)\n",
    "#bx, by = random_batch(x, y)\n",
    "loss = model.loss(x.to(dev), y.to(dev))\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Generated: {model.generate_str()}\")\n",
    "print(f\"Params: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# create interactive loss plot\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def train_model(model, lr, batches=400, block_size: int = 8, batch_size: int = 4):\n",
    "    # set model to train mode\n",
    "    model.train()\n",
    "\n",
    "    # create the plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    ax.set_xlabel(\"Batch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"Training loss\")\n",
    "\n",
    "    # train the model\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(batches):\n",
    "        bx, by = random_batch(train_data, block_size, batch_size)\n",
    "        loss = model.loss(bx, by)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # plot the loss as a line\n",
    "        ax.set_title(f\"Training loss (batch {epoch}/{batches})\")\n",
    "        ax.plot(epoch, loss.item(), \"r.\")\n",
    "        clear_output(wait=True)\n",
    "        display(fig)\n",
    "\n",
    "    # generate test loss\n",
    "    tx, ty = random_batch(test_data, block_size, 100)\n",
    "    test_loss = model.loss(tx, ty)\n",
    "    print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "train_model(model, .1, batches=200)\n",
    "for i in range(10):\n",
    "    print(f\"--- GENERATING {i} ---\")\n",
    "    print(model.generate_str())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Self Attention**\n",
    "\n",
    "We want to define a self attention head step by step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 8])\n",
      "score q: torch.Size([4, 8, 16])\n",
      "score tp: torch.Size([4, 16, 8])\n",
      "Diff is: True | 1.023709774017334e-05\n",
      "-- Out --\n",
      "torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "channels = 32\n",
    "head_size = 16\n",
    "\n",
    "# define the input\n",
    "x = torch.randn(batch_size, block_size, channels)  # (B, T, C)\n",
    "\n",
    "# define the key and query vectors\n",
    "k = nn.Linear(channels, head_size)  # (C, H) - Weigths\n",
    "q = nn.Linear(channels, head_size)  # (C, H) - Weights\n",
    "v = nn.Linear(channels, head_size)  # (C, H) - Weights\n",
    "\n",
    "# generate the embedding vectors\n",
    "xk = k(x)   # (B, T, H)\n",
    "xq = q(x)   # (B, T, H)\n",
    "\n",
    "# compute score manually\n",
    "# NOTE: score should generate a vector of shape (T) for each batch element (so (B, T))\n",
    "mscore = torch.zeros((batch_size, block_size, block_size), dtype=torch.float32)\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        # compute the score for the current time step\n",
    "        for r in range(t+1):\n",
    "            mscore[b, t, r] = xq[b, t, :].dot(xk[b, r, :])\n",
    "print(mscore.shape)\n",
    "\n",
    "print(f\"score q: {xq.shape}\")\n",
    "print(f\"score tp: {xk.transpose(-1, -2).shape}\")\n",
    "wei = xq @ xk.transpose(-1, -2)  # (B, T, T) - Multiply along the embedding dimension (B, T, H) @ (B, H, T) -> (B, T, T)\n",
    "\n",
    "# apply tril mask\n",
    "tril = torch.tril(torch.ones((block_size, block_size), dtype=torch.float32))\n",
    "score = wei.masked_fill(tril == 0, 0)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "# divide by key data length (used for more stable gradients)\n",
    "# NOTE: this will stabilize gradients of softmax\n",
    "wei = wei / head_size**-.5 #np.sqrt(head_size)\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "xv = v(x)  # (B, T, H)\n",
    "out = wei @ xv\n",
    "#score = (xq * xk).sum(-1)  # (B, T)\n",
    "#print(score.shape)\n",
    "\n",
    "print(f\"Diff is: {torch.allclose(mscore, score)} | {(mscore - score).abs().sum()}\")\n",
    "#assert torch.allclose(mscore, score)\n",
    "\n",
    "print(\"-- Out --\")\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.3193, -0.6377, -1.0888, -0.1635, -1.9530, -2.2560, -0.7793,  0.2158,\n",
      "         0.2549, -2.4271])\n",
      "tensor([0.0504, 0.0996, 0.0634, 0.1600, 0.0267, 0.0197, 0.0864, 0.2338, 0.2432,\n",
      "        0.0166])\n",
      "tensor([1.9200e-06, 4.4802e-04, 1.2133e-05, 1.9902e-02, 1.2069e-08, 1.0687e-09,\n",
      "        1.4436e-04, 4.1366e-01, 5.6583e-01, 2.7184e-10])\n"
     ]
    }
   ],
   "source": [
    "# test softmax variance and peakyness\n",
    "x = torch.randn(10)\n",
    "print(x)\n",
    "print(F.softmax(x, dim=-1))\n",
    "print(F.softmax(x*8, dim=-1))\n",
    "\n",
    "# Notice that this pushes more weight into the highest element (since the math behind softmax is exponential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# create an attention head module\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mAttentionHead\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m      3\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, emb_size: \u001b[39mint\u001b[39m, head_size: \u001b[39mint\u001b[39m, encoder: \u001b[39mbool\u001b[39m, block_size: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m, dropout: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m      4\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# create an attention head module\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, emb_size: int, head_size: int, encoder: bool, block_size: int = 32, dropout: float = None):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.head_size = head_size\n",
    "        self.encoder = encoder\n",
    "\n",
    "        # define the key, query, and value vectors\n",
    "        self.k = nn.Linear(emb_size, head_size)\n",
    "        self.q = nn.Linear(emb_size, head_size)\n",
    "        self.v = nn.Linear(emb_size, head_size)\n",
    "\n",
    "        # check for dropout\n",
    "        if dropout is not None:\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # create register buffer for the tril\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones((block_size, block_size), dtype=torch.float32)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # compute key and query vectors\n",
    "        xk = self.k(x)  # (B, T, H)\n",
    "        xq = self.q(x)  # (B, T, H)\n",
    "\n",
    "        # compute weight by transpose and matmul product\n",
    "        wei = xq @ xk.transpose(-1, -2)\n",
    "\n",
    "        # apply tril mask\n",
    "        if self.encoder:\n",
    "            wei = wei.masked_fill(self.tril == 0, float('-inf'))\n",
    "        \n",
    "        # normalize weights and check for dropout\n",
    "        wei = F.softmax(wei / self.head_size**-.5, dim=-1)\n",
    "        if hasattr(self, \"dropout\"):\n",
    "            wei = self.dropout(wei)\n",
    "\n",
    "        # normalize data and apply softmax\n",
    "        xv = self.v(x)  # (B, T, H)\n",
    "        out = wei @ xv\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiAttentionHead(nn.Module):\n",
    "    def __init__(self, num_heads: int, emb_size: int, head_size: int, block_size: int, encoders: bool, dropout: float = None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([AttentionHead(emb_size, head_size, block_size=block_size, encoder=encoders, dropout=dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, emb_size)\n",
    "        if dropout is not None:\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # compute the attention for each head\n",
    "        outs = [head(x) for head in self.heads]\n",
    "\n",
    "        # concatenate the outputs\n",
    "        out = torch.cat(outs, dim=-1)\n",
    "\n",
    "        # project back to the original embedding size\n",
    "        out = self.proj(out)\n",
    "\n",
    "        # apply dropout\n",
    "        if hasattr(self, \"dropout\"):\n",
    "            out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_size: int, num_heads: int, encoders: bool, block_size: int, head_size: int = None, proj_size: int = None, dropout: float = None):\n",
    "        super().__init__()\n",
    "\n",
    "        # compute the headsize for each head\n",
    "        head_size = head_size or (emb_size // num_heads)\n",
    "        proj_size = proj_size or (head_size * num_heads * 4)\n",
    "\n",
    "        # mutli-head attention\n",
    "        self.att_head = MultiAttentionHead(num_heads, emb_size, head_size, block_size=block_size, encoders=encoders, dropout=dropout)\n",
    "\n",
    "        # compute the relu layer\n",
    "        self.relu_layer = nn.Sequential(\n",
    "            nn.Linear(head_size * num_heads, proj_size),\n",
    "            nn.ReLU(),\n",
    "            # projection layer back to regular size\n",
    "            nn.Linear(proj_size, emb_size),\n",
    "            # dropout to regularize\n",
    "            *([nn.Dropout(dropout)] if dropout is not None else [])\n",
    "        )\n",
    "\n",
    "        # normalization\n",
    "        self.norm1 = nn.LayerNorm(emb_size)\n",
    "        self.norm2 = nn.LayerNorm(emb_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # compute the attention\n",
    "        #out = self.att_head(x)\n",
    "        out = self.att_head(self.norm1(x)) + x\n",
    "\n",
    "        # adding\n",
    "        #out = self.norm1(out + x)\n",
    "\n",
    "        # compute the relu layer\n",
    "        #out = self.relu_layer(out)\n",
    "        out = self.relu_layer(self.norm2(out)) + out\n",
    "\n",
    "        # adding\n",
    "        #out = self.norm2(out + x)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# build a model around the attention head\n",
    "class TransformerLight(LModel):\n",
    "    def __init__(self, vocab_size, emb_size=128, heads=4, depth=6, block_size=32, dropout=0.1):\n",
    "        super().__init__(vocab_size)\n",
    "\n",
    "        # define the embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.pos_embedding = nn.Embedding(block_size, emb_size)\n",
    "        # self.att_head = AttentionHead(emb_size, vocab_size, encoder=True)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(emb_size, heads, block_size=block_size, encoders=True, dropout=dropout) for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(emb_size)\n",
    "        self.lm_head = nn.Linear(emb_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # validate device\n",
    "        if self.device != x.device:\n",
    "            x = x.to(self.device)\n",
    "\n",
    "        # get ids\n",
    "        B, T = x.shape\n",
    "\n",
    "        # retrieve the embeddings\n",
    "        chars = self.embedding(x)\n",
    "        pos = self.pos_embedding(torch.arange(T, device=x.device))\n",
    "        # add embeddings\n",
    "        embs = chars + pos\n",
    "\n",
    "        # compute attention\n",
    "        att = embs\n",
    "        for blck in self.blocks:\n",
    "            att = blck(att)\n",
    "        \n",
    "        # normalize\n",
    "        att = self.norm(att)\n",
    "\n",
    "        # compute logits\n",
    "        logits = self.lm_head(att)\n",
    "        return logits\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        # validate device\n",
    "        if self.device != y.device:\n",
    "            y = y.to(self.device)\n",
    "\n",
    "        # compute the logits\n",
    "        logits = self.forward(x)\n",
    "\n",
    "        # requires a reshape from (B, T, vocab_size) to (B*T, vocab_size)\n",
    "        logits = logits.view(-1, self.vocab_size)\n",
    "        target = y.view(-1)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = F.cross_entropy(logits, target)\n",
    "        return loss\n",
    "    \n",
    "# hparams\n",
    "t_block_size = 256\n",
    "t_batch_size = 8\n",
    "t_heads = 6\n",
    "t_emb_size = 384\n",
    "t_blocks = 8\n",
    "t_dropout = 0.2\n",
    "t_lr = 3e-4\n",
    "\n",
    "# create the model\n",
    "model = TransformerLight(vocab_size, emb_size=t_emb_size, heads=t_heads, depth=t_blocks, block_size=t_block_size, dropout=t_dropout)\n",
    "model.to(torch_device)\n",
    "print(f\"Params: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "train_model(model, t_lr, batches=2000, block_size=t_block_size, batch_size=t_batch_size)\n",
    "for i in range(5):\n",
    "    print(f\"--- Generated {i} ---\")\n",
    "    print(model.generate_str())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7fe1a4422759eafe6868ccc54152cc839a79337f6f12136ad2d52cc24128f952"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
